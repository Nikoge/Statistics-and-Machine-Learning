---
title: "BDA2 Labr Report"
author: "Andreas Stasinakis(andst745) & Mim Kemal Tekin(mimte666)"
date: "May 10, 2019"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Task 1

In this task we used temperatures-big.csv. First we find minimum and maximum temperatures for each year. We can see results of it as following:

## Data import

```{r, eval = FALSE}

# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

```

## Task 1: Maximum and Minimum Temperature for each Year

```{r, eval = FALSE}
# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
tempReadingsString= ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")


############
#####   1
############
df_year_station = schemaTempReadings. \
    where("year >= 1950 and year <= 2014"). \
    groupBy("year")

df_year_station_max = df_year_station. \
    agg(F.max("value").alias("max_temp")). \
    orderBy("max_temp", ascending=False)

df_year_station_min = df_year_station. \
    agg(F.min("value").alias("min_temp")). \
    orderBy("min_temp", ascending=False)


df_year_station_max.rdd.saveAsTextFile("./bda2_res/t1_max_stations")
df_year_station_min.rdd.saveAsTextFile("./bda2_res/t1_min_stations")


############
#####   1a
############
df_year_station = schemaTempReadings. \
    where("year >= 1950 and year <= 2014"). \
    groupBy("year", "station")

df_year_station_max = df_year_station. \
    agg(F.max("value").alias("max_temp")). \
    orderBy("max_temp", ascending=False)

df_year_station_min = df_year_station. \
    agg(F.min("value").alias("min_temp")). \
    orderBy("min_temp", ascending=False)

df_year_station_max.rdd.saveAsTextFile("./bda2_res/t1_max_with_stations")
df_year_station_min.rdd.saveAsTextFile("./bda2_res/t1_min_with_stations")

```



**Maximum Temperature**

\begin{center}
    \includegraphics[width=80px]{./img/t1_max_stations.png}
\end{center}


**Minimum Temperature**

\begin{center}
    \includegraphics[width=80px]{./img/t1_min_stations.png}
\end{center}


**Maximum Temperature with station**

\begin{center}
    \includegraphics[width=80px]{./img/t1_max_with_stations.png}
\end{center}


**Minimum Temperature with station**

\begin{center}
    \includegraphics[width=80px]{./img/t1_min_with_stations.png}
\end{center}

# Task 2

*Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees. Repeat the exercise, this time taking only distinct readings from each station. That is, if a station reported a reading above 10 degrees in some month, then it appears only once in the count for that month.*  
*In this exercise you will use the temperature-readings.csv file. The output should contain the following information:*  
**Year, month, count**

```{r, eval = FALSE}
# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
tempReadingsString= ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")



############
#####   2.i & ii with API
############
counts_monthly_api = schemaTempReadings.\
    where("value>=10 and year>=1950 and year<=2014").\
    groupby("year", "month").\
    agg(F.count("value").alias("count")).\
    orderBy("count", ascending=False)
    # orderBy("count", ascending=False)

counts_monthly_distinct_api = schemaTempReadings.\
    where("value>=10 and year>=1950 and year<=2014").\
    select("year", "month", "station", "value").\
    distinct().\
    groupby("year", "month").\
    agg(F.count("value").alias("count")).\
    orderBy("count", ascending=False)
    # orderBy("count", ascending=False)

counts_monthly_api.rdd.saveAsTextFile("./bda2_res/t2_api_count_reads_month")
counts_monthly_distinct_api.rdd.saveAsTextFile("./bda2_res/t2_api_count_reads_month_distinct")

############
#####   2.i & ii with SQL
############

q1 ="select year, month, count(value) as count \
from tempReadingsTable \
where value>=10 and year between 1950 and 2014 \
group by year, month \
order by count desc"
# order by count desc"

q2 = "SELECT year, month, count(value) as count \
FROM (SELECT distinct year, month, station, value \
FROM tempReadingsTable \
WHERE value >= 10 and year BETWEEN 1950 AND 2014) AS Q \
group by year, month \
order by count desc"
# order by count desc"

counts_monthly = sqlContext.sql(q1)

counts_monthly_distinct = sqlContext.sql(q2)

counts_monthly.rdd.saveAsTextFile("./bda2_res/t2_sql_count_reads_month")
counts_monthly_distinct.rdd.saveAsTextFile("./bda2_res/t2_sql_count_reads_month_distinct")


```

### Results with API
**Monthly Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_api_count_reads_month.png}
\end{center}

**Monthly Distinct Station Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_api_count_reads_month_distinct.png}
\end{center}


### Results with SQL 
**Monthly Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_sql_count_reads_month.png}
\end{center}

**Monthly Distinct Station Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_sql_count_reads_month_distinct.png}
\end{center}


# Task 3

*Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960- 2014. Bear in mind that not every station has the readings for each month in this timeframe. In this exercise you will use the temperature-readings.csv file.*  
*The output should contain the following information:*  
**Year, month, station number, average monthly temperature**

```{r, eval= FALSE}

# define headers of the dataframe
tempReadingsString= ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")

avg_monthly = schemaTempReadings.\
    where("year>=1960 and year<=2014").\
    groupby("date", "year", "month", "station").\
    agg(((F.max("value") + F.min("value"))/2).alias("avg_daily")).\
    groupby("year", "month", "station").\
    agg(F.avg("avg_daily").alias("avg_monthly")).\
    orderBy("year", "month", ascending=False)
    # orderBy("avg_daily", ascending=False)

avg_monthly.rdd.saveAsTextFile("./bda2_res/t3_avg_month")

```


\begin{center}
    \includegraphics[width=180px]{./img/t3_avg_month.png}
\end{center}



# Task 4

*Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200 mm. In this exercise you will use the temperature-readings.csv and precipitation-readings.csv files.*  
*The output should contain the following information:*  

**Station number, maximum measured temperature, maximum daily precipitation**

```{r, eval = FALSE}
# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
tempReadingsString = ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))


# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")

# import the data
# precipitation_file = sc.textFile("../station_data/short_precipitation-readings.csv")
precipitation_file = sc.textFile("/user/x_mimte/data/precipitation-readings.csv")

# transform the data by splitting each line
lines_precipitation = precipitation_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
precipitationString= ["station", "date", "year", "month", "time", "value", "quality"]

precipitationRow = lines_precipitation.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

schemaPrecipitation = sqlContext.createDataFrame(precipitationRow, precipitationString)
schemaPrecipitation.registerTempTable("precipitationtable")



max_temp = schemaTempReadings.\
    groupby("station").\
    agg(F.max("value").alias("max_temp")).\
    where("max_temp>=25 and max_temp<=30")



max_daily_prec = schemaPrecipitation.\
    groupby("date", "station").\
    agg(F.sum("value").alias("daily_prec")).\
    groupby("station").\
    agg(F.max("daily_prec").alias("max_prec")).\
    where("max_prec>100 and max_prec<200")

max_values = max_temp.\
    join(max_daily_prec, "station")

max_values.rdd.saveAsTextFile("./bda2_res/t4_station_temp_prec")
```

The output is an empty file.

# Task 5

*Calculate the average monthly precipitation for the Ostergotland region (list of stations is provided in the separate file) for the period 1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).*  
*In this exercise you will use the precipitation-readings.csv and stations-Ostergotland.csv files.*  
*The output should contain the following information:*  
**Year, month, average monthly precipitation**

```{r, eval = FALSE}
# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
tempReadingsString = ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))


# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")

# import the data
# precipitation_file = sc.textFile("../station_data/short_precipitation-readings.csv")
precipitation_file = sc.textFile("/user/x_mimte/data/precipitation-readings.csv")

# transform the data by splitting each line
lines_precipitation = precipitation_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
precipitationString= ["station", "date", "year", "month", "time", "value", "quality"]

precipitationRow = lines_precipitation.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

schemaPrecipitation = sqlContext.createDataFrame(precipitationRow, precipitationString)
schemaPrecipitation.registerTempTable("precipitationtable")


# import stations in Ostergotland
# ost_station_file = sc.textFile("../station_data/stations-Ostergotland.csv")
ost_station_file = sc.textFile("/user/x_mimte/data/stations-Ostergotland.csv")

# transform the data by splitting each line
lines_ost = ost_station_file. \
    map(lambda line: line.split(";"))

# map the data for our headers
# We could add all other features too. But we do not have any question
# which asks for all other informations.
ostStationsString = ["station", "station_name"]

ostStationsRow = lines_ost.map(lambda line: (line[0], line[1]))


schemaOstStations = sqlContext.createDataFrame(ostStationsRow, ostStationsString)
schemaOstStations.registerTempTable("oststations")



############
#####   5
############

ostStations = schemaOstStations.select("station")

#another way
# avg_monthly_long = schemaOstStations.\
#     join(schemaPrecipitation, "station").\
#     where("year>1993 and year<2016").\
#     groupby("year", "month", "station").\
#     agg(F.avg("value").alias("avg_prec")).\
#     groupBy("year", "month").\
#     agg(F.avg("avg_prec").alias("avg_prec")).\
#     orderBy("year", "month", ascending=False)
#

avg_monthly = schemaOstStations.\
    join(schemaPrecipitation, "station").\
    where("year>1993 and year<2016").\
    groupby("year", "month").\
    agg(F.avg("value").alias("avg_prec")).\
    orderBy("year", "month", ascending=False)

# avg_monthly_long.rdd.saveAsTextFile("./bda2_res/t5_avg_ost_station_long")
avg_monthly.rdd.saveAsTextFile("./bda2_res/t5_avg_ost_station")
```

\begin{center}
    \includegraphics[width=140px]{./img/t5_avg_ost_station.png}
\end{center}


# Task 6

*Compare the average monthly temperature (find the difference) in the period 1950-2014 for all stations in Ostergotland with long-term monthly averages in the period of 1950-1980. Make a plot of your results.*  
*The output should contain the following information:*  
**Year, month, difference**

```{r, eval = FALSE}
# import libraries
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row
from pyspark.sql import functions as F

# create Contexts
sc = SparkContext()
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a tuple.
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
# temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# define headers of the dataframe
tempReadingsString= ["station", "date", "year", "month", "time", "value", "quality"]

# map the data for our headers
# ['103100', '1996-07-06', '15:00:00', '14.8', 'G']
tempReadingsRow = lines.map(lambda line: (line[0], line[1], int(line[1][0:4]), \
                            int(line[1][5:7]), line[2], \
                            float(line[3]), line[4]))

# Apply the schema to the RDD.
schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow, tempReadingsString)
schemaTempReadings.registerTempTable("tempReadingsTable")

# import stations in Ostergotland
# ost_station_file = sc.textFile("../station_data/stations-Ostergotland.csv")
ost_station_file = sc.textFile("/user/x_mimte/data/stations-Ostergotland.csv")

# transform the data by splitting each line
lines_ost = ost_station_file. \
    map(lambda line: line.split(";"))

# map the data for our headers
# We could add all other features too. But we do not have any question
# which asks for all other informations.
ostStationsString = ["station", "station_name"]

ostStationsRow = lines_ost.map(lambda line: (line[0], line[1]))


schemaOstStations = sqlContext.createDataFrame(ostStationsRow, ostStationsString)
schemaOstStations.registerTempTable("oststations")


############
#####   6
############
avg_monthly50_14 = schemaOstStations.\
    join(schemaTempReadings, "station").\
    where("year>=1950 and year<=2014").\
    groupby("date", "year", "month", "station").\
    agg(((F.max("value") + F.min("value"))/2).alias("avg_daily")).\
    groupby("year", "month").\
    agg(F.avg("avg_daily").alias("avg_monthly")).\
    orderBy("year", "month", ascending=False)
    # orderBy("avg_daily", ascending=False)

long_term_avg = avg_monthly50_14.\
    where("year<=1980").\
    groupby("month").\
    agg(F.avg("avg_monthly").alias("avg")).\
    orderBy("month", ascending=False)


avg_monthly50_14.rdd.saveAsTextFile("./bda2_res/t6_avg_monthly")
long_term_avg.rdd.saveAsTextFile("./bda2_res/t6_long_term_avg")

```

**Average Monthly**

\begin{center}
    \includegraphics[width=140px]{./img/t6_avg_monthly.png}
\end{center}

**Long Term Average Monthly**

\begin{center}
    \includegraphics[width=110px]{./img/t6_long_term_avg.png}
\end{center}
