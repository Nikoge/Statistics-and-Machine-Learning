---
title: "BDA1-Spark - Lab Report"
author: "Mim Kemal Tekin (mimte666) & Andreas Stasinakis (andst745)"
date: "4/30/2019"
output: 
  pdf_document:
    toc: true
---

<style>
.myChunk {
  background-color: red;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 1

In this task we used temperatures-big.csv. First we find minimum and maximum temperatures for each year. We can see results of it as following:

## Data import

```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))
```


## Task 1: Maximum and Minimum Temperature for each Year

```{r eval=F}
# transform the data by extracting year and temperature as tuple
year_temperature = lines. \
    map(lambda x: (x[1][0:4], float(x[3])))

# filter data by year
year_temperature = year_temperature. \
    filter(lambda x: int(x[0])>=1950 and int(x[0])<=2014)

# reducer, to get the max temperature by KEY (year)
max_temperatures = year_temperature. \
    reduceByKey(max). \
    sortByKey(ascending=False)
# reducer, to get the min temperature by KEY (year)
min_temperatures = year_temperature. \
    reduceByKey(min). \
    sortByKey(ascending=False)

max_temperatures.saveAsTextFile("./res/t1_max")
min_temperatures.saveAsTextFile("./res/t1_min")
```

**Maximum Temperature**

\begin{center}
    \includegraphics[width=80px]{./img/t1_max.png}
\end{center}


**Minimum Temperature**

\begin{center}
    \includegraphics[width=80px]{./img/t1_min.png}
\end{center}


## Task 1.a: Maximum and Minimum Temperatures For each Year Included Station Numbers

In this task we are asked for extending previous task to find max and min temperatures for each year and which station measured these values.

```{r eval=F}
year_station_temp = lines. \
    map(lambda x: (x[1][0:4], (x[0], float(x[3]))))

year_temperature = year_station_temp. \
    filter(lambda x: int(x[0])>=1950 and int(x[0])<=2014)

max_temperatures = year_temperature. \
    reduceByKey(lambda a,b: a if a>b else b). \
    sortByKey(False)

min_temperatures = year_temperature. \
    reduceByKey(lambda a,b: b if a<b else a). \
    sortByKey(False)

max_temperatures.saveAsTextFile("./res/t1_max_stations")
min_temperatures.saveAsTextFile("./res/t1_min_stations")
```

**Maximum Temperature**

\begin{center}
    \includegraphics[width=100px]{./img/t1_max_stations.png}
\end{center}


**Minimum Temperature**

\begin{center}
    \includegraphics[width=100px]{./img/t1_min_stations.png}
\end{center}


## Task 1.b

Now we run a benchmark test to see how much time non-parallel script that we wrote takes when we compare with parallel one. The script finds only max temperatures for each year. We run this script on "temperatures-big.csv". After running non-parallel script we get the results below. 

\begin{center}
    \includegraphics[width=100px]{./img/t1b_nonpar_runtime.png}
\end{center}

And we can see it took 1799 seconds, which is close to 30 minutes. This script has a for loop and it traverses each row that the data file has and in every iteration it checks for the year filter that we have and checks if it is max or not.  
Also we modified the script that we wrote in previous task and we run a benchmark for this task. This script uses hdfs filesystem and it runs as parallel processing. We get the result as following:

\begin{center}
    \includegraphics[width=100px]{./img/t1b_par_runtime.png}
\end{center}

This number presents seconds as before and it is close to 4 minutes. We can clearly see a performance difference. Parallel scipt finishes the task approximately 7 times faster than the normal script. The time above presents only the runtime of main algorithm. We can check the real starting time and end time of the process included with the time which includes queue time for scheduling on server and printing the results to the files below:

\begin{center}
    \includegraphics[width=450px]{./img/t1b_par_start.png}
\end{center}
\begin{center}
    \includegraphics[width=450px]{./img/t1b_par_end.png}
\end{center}

### Non-Parallel Script

```{r eval=F}
# - *- coding: utf- 8 - *-
# set year filter boundaries
lower_year = 1950
upper_year = 2014
# create empty dictionary for our response
response_max = {year:-10000 for year in range(lower_year,upper_year+1)}

line_no = 0
# read file
with open("/nfshome/hadoop_examples/shared_data/temperatures-big.csv","r") as file:
    for line in file:
        # We split the line by ; symbol.
        # after spliting er get the format as following:
        # ["station_no", "yyyy-mm-dd", "hh:mm:ss", "temp", "G new_line"]
        #Â we will check years and find the max and min of each year
        if not line_no%10000:
            print(line_no)
        line_no += 1
        reading = line.split(";")
        year = int(reading[1][0:4])
        # check our filter
        if year > 1950 and year < 2014:
            temp = float(reading[3])
            if temp > response_max[year]:
                response_max[year] = temp

end = time.time()

print "RUNTIME = {}".format(end-start)
with open("task1b_nonparallel_time.out","w") as file:
	file.write("RUNTIME = {}".format(end-start))            

with open("t1b_max_stations", "w") as file:
    for k in response_max.keys():
        file.write("%s: %s\n" % (k, response_max[k]))
```

### Parallel Script

```{r eval=F}
# import pyspark
from pyspark import SparkContext
import time

start = time.time()
# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperatures-big.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))


year_temperature = lines. \
    map(lambda x: (x[1][0:4], float(x[3]))). \
    filter(lambda x: int(x[0])>=1950 and int(x[0])<=2014). \
    reduceByKey(max). \
    sortByKey(ascending=False)


end = time.time()

print("RUNTIME = {}".format(end - start))


max_temperatures.saveAsTextFile("./res/t1_max")
```


# Task 2

*Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees. Repeat the exercise, this time taking only distinct readings from each station. That is, if a station reported a reading above 10 degrees in some month, then it appears only once in the count for that month.*  
*In this exercise you will use the temperature-readings.csv file. The output should contain the following information:*  
**Year, month, count**




```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))
    
############
#####   2.i
############

# map as ((year, month), (station_no, temp, 1))
year_month = lines. \
    map(lambda x: ((x[1][0:4],x[1][5:7]), (x[0], float(x[3]))))

# filter by constraints and put 1 as value
year_month = year_month. \
    filter(lambda x: int(x[0][0])>=1950 and int(x[0][0])<=2014 and x[1][1]>10)

# map as pair rdd (key,1) and count them, sort them
count_reads = year_month. \
    map(lambda x: (x[0], 1)). \
    reduceByKey(lambda x,y: x+y). \
    sortByKey(ascending=False)

count_reads.saveAsTextFile("./res/t2_count_reads_month")


############
#####   2.ii
############
# get only one tuple for a station in one month.
# Remove duplicated reads in one month
year_month_unique = year_month. \
    map(lambda x: (x[0], (x[1][0], 1))). \
    distinct()

# map as pair rdd (key,1) and count them, sort them
station_month_counts = year_month_unique. \
    map(lambda x: (x[0], 1)). \
    reduceByKey(lambda x,y: x+y). \
    sortByKey(ascending=False)

count_reads.saveAsTextFile("./res/t2_count_reads_month_distinct")

```

**Monthly Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_count_reads_month.png}
\end{center}

**Monthly Distinct Station Reading Count Greater than 10**

\begin{center}
    \includegraphics[width=100px]{./img/t2_count_reads_month_distinct.png}
\end{center}

# Task 3

*Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960- 2014. Bear in mind that not every station has the readings for each month in this timeframe. In this exercise you will use the temperature-readings.csv file.*  
*The output should contain the following information:*  
**Year, month, station number, average monthly temperature**

```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))


############
#####   3
############

# map as (date, station_no), (temperature)
# to calculate average temp of each day
yr_mn_st = lines. \
    map(lambda x: ((x[1], x[0]), (float(x[3]))))

# calculate avg temp for each day by using defined formula
# avg of min of day and max of day
# we grouped it by key in order to apply function to days for each station seperately
daily_avg = yr_mn_st.groupByKey().mapValues(lambda x: (max(x)+min(x))/2)

# calculate average of month for each station
# map as (year, month, station_no), (daily_avg, 1)
# 1 for counting element count while summing
# sum temperature and count how many elements we have
# map it again to find the average
monthly_avg = daily_avg. \
    map(lambda x: ((x[0][0][0:4], x[0][0][5:7], x[0][1]), (x[1],1))). \
    reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1])). \
    map(lambda x: (x[0], x[1][0]/x[1][1])). \
    sortByKey(False)

monthly_avg.filter(lambda x: int(x[0][0])>1960 and int(x[0][0])<2014). \
    saveAsTextFile("./res/t3_avg_month")
```

\begin{center}
    \includegraphics[width=180px]{./img/t3_avg_month.png}
\end{center}

# Task 4

*Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200 mm. In this exercise you will use the temperature-readings.csv and precipitation-readings.csv files.*  
*The output should contain the following information:*  

**Station number, maximum measured temperature, maximum daily precipitation**

```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# import the data
# precipitation_file = sc.textFile("../station_data/short_precipitation-readings.csv")
precipitation_file = sc.textFile("/user/x_mimte/data/precipitation-readings.csv")

# transform the data by splitting each line
lines_precipitation = precipitation_file. \
    map(lambda line: line.split(";"))


############
#####   4
############


# map as (station_no, temp)
# find maximum read of station
station_temp = lines. \
    map(lambda x: (x[0], float(x[3]))). \
    reduceByKey(max)


# map as ((station, date), precipitation)
# calculate daily precipitation
# map as (station, precipitation)
# find max precipitation of station
station_precipitation = lines_precipitation. \
    map(lambda x: ((x[0], x[1]), float(x[3]))). \
    reduceByKey(lambda x,y: x+y). \
    map(lambda x: (x[0][0], x[1])). \
    reduceByKey(max)


# join them
station_temp_prec = station_temp.join(station_precipitation)

# filter the last data for constraints
station_temp_prec = station_temp_prec. \
    filter(lambda x: x[1][0]>25 and x[1][0]<30 \
    and x[1][1]>100 and x[1][1]<200)

station_temp_prec.saveAsTextFile("res/t4_station_temp_prec")
```

\begin{center}
    \includegraphics[width=100px]{./img/t4_station_temp_prec.png}
\end{center}


# Task 5

*Calculate the average monthly precipitation for the Ostergotland region (list of stations is provided in the separate file) for the period 1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).*  
*In this exercise you will use the precipitation-readings.csv and stations-Ostergotland.csv files.*  
*The output should contain the following information:*  
**Year, month, average monthly precipitation**

```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))

# import the data
# precipitation_file = sc.textFile("../station_data/short_precipitation-readings.csv")
precipitation_file = sc.textFile("/user/x_mimte/data/precipitation-readings.csv")

# transform the data by splitting each line
lines_precipitation = precipitation_file. \
    map(lambda line: line.split(";"))


# import stations in Ostergotland
# ost_station_file = sc.textFile("../station_data/stations-Ostergotland.csv")
ost_station_file = sc.textFile("/user/x_mimte/data/stations-Ostergotland.csv")


############
#####   5
############

# transform the data by splitting each line
lines_ost = ost_station_file. \
    map(lambda line: line.split(";"))

# get station_ids in Ostergotland as an Array
ost_station_ids = lines_ost. \
    map(lambda x: x[0]).collect()

# map as ((station_no, yyyy, mm), (precipitation, 1))
# filter only ostergotland stations and date constraint
# map as ((yyyy, mm), (precipitation, 1))
# sum all values by reduceByKey
# map it again to find avg of month
# sort
ost_station_prec = lines_precipitation. \
    map(lambda x: ((x[0], x[1][0:4], x[1][5:7]), (float(x[3]), 1))). \
    filter(lambda x: x[0][0] in ost_station_ids and \
        int(x[0][1])>1993 and int(x[0][1])<2016). \
    map(lambda x: ((x[0][1], x[0][2]), x[1])). \
    reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])). \
    map(lambda x: (x[0], x[1][0]/x[1][1])). \
    sortByKey(False)

ost_station_prec.saveAsTextFile("./res/t5_avg_ost_station")
```
\begin{center}
    \includegraphics[width=140px]{./img/t5_avg_ost_station.png}
\end{center}


# Task 6

*Compare the average monthly temperature (find the difference) in the period 1950-2014 for all stations in Ostergotland with long-term monthly averages in the period of 1950-1980. Make a plot of your results.*  
*The output should contain the following information:*  
**Year, month, difference**


```{r eval=F}
# import pyspark
from pyspark import SparkContext

# create the spark application
sc = SparkContext(appName = "Exercise App")

# import the data
# temperature_file = sc.textFile("../station_data/short_temperature_reads.csv")
temperature_file = sc.textFile("/user/x_mimte/data/temperature-readings.csv")

# transform the data by splitting each line
lines = temperature_file. \
    map(lambda line: line.split(";"))


# import stations in Ostergotland
# ost_station_file = sc.textFile("../station_data/stations-Ostergotland.csv")
ost_station_file = sc.textFile("/user/x_mimte/data/stations-Ostergotland.csv")


# transform the data by splitting each line
lines_ost = ost_station_file. \
    map(lambda line: line.split(";"))

# get station_ids in Ostergotland as an Array
ost_station_ids = lines_ost. \
    map(lambda x: x[0]).collect()


############
#####   3
############

# map as (date, station_no), (temperature)
# to calculate average temp of each day
yr_mn_st = lines. \
    map(lambda x: ((x[1], x[0]), (float(x[3]))))

# calculate avg temp for each day by using defined formula
# avg of min of day and max of day
# we grouped it by key in order to apply function to days for each station seperately
daily_avg = yr_mn_st.groupByKey().mapValues(lambda x: (max(x)+min(x))/2)

# calculate average of month for each station
# map as (year, month, station_no), (daily_avg, 1)
# filter them in order to get only ostergotland stations
# 1 for counting element count while summing
# sum temperature and count how many elements we have
# map it again to find the average
monthly_avg = daily_avg. \
    map(lambda x: ((x[0][0][0:4], x[0][0][5:7], x[0][1]), (x[1],1))). \
    filter(lambda x: x[0][2] in ost_station_ids). \
    reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1])). \
    map(lambda x: (x[0], x[1][0]/x[1][1])). \
    sortByKey(False)



############
#####   6
############

# we use the avg temperature monthly that we found before in taks 3
# filter for the year constraint
# map as ((yyyy, mm),(avg_temp, 1))
# reduceByKey and found sum of values by keys
# map it again as ((yyyy, mm), avg) where avg is value[0]/value[1]
# sort them
monthly_avg50_14 = monthly_avg. \
    filter(lambda x: int(x[0][0])>=1950 and int(x[0][0])<=2014). \
    map(lambda x: ((x[0][0], x[0][1]),(x[1], 1))). \
    reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])). \
    map(lambda x: (x[0], x[1][0]/x[1][1])). \
    sortByKey(ascending=False)

# filter again to get only before 1980
# map it as (mm, (avg_temp, 1))
# reduceByKey and found sum of values by keys
# map it again as (mm, avg)
# sort them
long_term_avg = monthly_avg50_14. \
    filter(lambda x: int(x[0][0])<=1980). \
    map(lambda x: (x[0][1], (x[1], 1))). \
    reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])). \
    map(lambda x: (x[0], x[1][0] / x[1][1])). \
    sortByKey(ascending=False)


monthly_avg50_14.saveAsTextFile("res/t6_avg_monthly")
long_term_avg.saveAsTextFile("res/t6_long_term_avg")
```

**Average Monthly**

\begin{center}
    \includegraphics[width=140px]{./img/t6_avg_monthly.png}
\end{center}

**Long Term Average Monthly**

\begin{center}
    \includegraphics[width=110px]{./img/t6_long_term_avg.png}
\end{center}
