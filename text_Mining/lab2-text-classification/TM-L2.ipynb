{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the task of sorting text documents into predefined classes. The concrete problem you will be working on in this lab is the classification of texts with respect to their political affiliation. The specific texts you are going to classify are speeches held in the [Riksdag](https://www.riksdagen.se/en/), the Swedish national legislature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for this lab comes from [The Riksdag’s Open Data](https://data.riksdagen.se/in-english/). We have tokenized the speeches and put them into two compressed [JSON](https://en.wikipedia.org/wiki/JSON) files:\n",
    "\n",
    "* `speeches-201718.json.bz2` (speeches from the 2017/2018 parliamentary session)\n",
    "* `speeches-201819.json.bz2` (ditto, from the 2018/2019 session)\n",
    "\n",
    "We start by loading these files into two separate data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries we need \n",
    "import pandas as pd\n",
    "import bz2\n",
    "import numpy as np\n",
    "\n",
    "#load and store the data from a json form\n",
    "with bz2.open(\"speeches-201718.json.bz2\") as source:\n",
    "    speeches_201718 = pd.read_json(source)\n",
    "\n",
    "with bz2.open(\"speeches-201819.json.bz2\") as source:\n",
    "    speeches_201819 = pd.read_json(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the two data frames, you can see that there are three labelled columns: `id` (the official speech ID), `words` (the space-separated words of the speech), and `party` (the party of the speaker, represented by its customary abbreviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H5-002-004</td>\n",
       "      <td>eders majestäter eders kungliga högheter herr ...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H5-003-001</td>\n",
       "      <td>aktuell debatt om situationen för ensamkommand...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H5-003-002</td>\n",
       "      <td>herr talman och ledamöter jag vill börja med a...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H5-003-003</td>\n",
       "      <td>herr talman åhörare den här debatten handlar a...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H5-003-004</td>\n",
       "      <td>herr talman ansvar och rättssäkerhet är två or...</td>\n",
       "      <td>SD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              words party\n",
       "0  H5-002-004  eders majestäter eders kungliga högheter herr ...     S\n",
       "1  H5-003-001  aktuell debatt om situationen för ensamkommand...     V\n",
       "2  H5-003-002  herr talman och ledamöter jag vill börja med a...     S\n",
       "3  H5-003-003  herr talman åhörare den här debatten handlar a...     M\n",
       "4  H5-003-004  herr talman ansvar och rättssäkerhet är två or...    SD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the head of the data in order to get an idea of the data\n",
    "speeches_201718.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the lab, we will be using the speeches from 2017/2018 as our training data, and the speeches from 2018/2019 as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test\n",
    "training_data, test_data = speeches_201718, speeches_201819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reference, we store the sorted list of party abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'KD', 'L', 'M', 'MP', 'S', 'SD', 'V']\n"
     ]
    }
   ],
   "source": [
    "#store the labels(classes)\n",
    "parties = sorted(training_data[\"party\"].unique())\n",
    "print(parties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to get to know the data better by plotting a simple visualization.\n",
    "\n",
    "If you are not familiar with the Swedish political system and the parties represented in the Riksdag in particular, then we suggest that you have a look at the Wikipedia article about the [2018 Swedish general election](https://en.wikipedia.org/wiki/2018_Swedish_general_election).\n",
    "\n",
    "For the lab, we ask you to compare the two data frames with respect to the distribution of the speeches over the different parties. Write code to generate two bar plots that visualize this information, one for the 2017/2018 speeches and one for the 2018/2019 speeches. Inspect the two plots, and compare them\n",
    "\n",
    "* to each other\n",
    "* to the results of the 2014 and the 2018 general elections\n",
    "\n",
    "Summarize your observations in a short text in the cell below.\n",
    "\n",
    "**Tip:** If you need help with creating bar plots, [Bar Plot using Pandas](https://dfrieds.com/data-visualizations/bar-plot-python-pandas) provides a useful tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the distribution of the classes\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = training_data[\"party\"].unique()\n",
    "y1 = training_data[\"party\"].value_counts()\n",
    "x2 = test_data[\"party\"].unique()\n",
    "y2 = test_data[\"party\"].value_counts()\n",
    "r1 = np.arange(len(y1))\n",
    "r2 = [x + 0.25 for x in r1]\n",
    "\n",
    "plt.bar(r1, y1, color='blue', edgecolor='white',width = 0.5,\n",
    " label='2017-2018')\n",
    "plt.bar(r2, y2, color='yellow', edgecolor='black', width = 0.5,\n",
    "label='2018-2019')\n",
    "plt.xlabel('Parties', fontweight='bold')\n",
    "plt.xticks([r + 0.25 for r in range(len(y1))], x1)\n",
    "\n",
    "plt.title(\"Speeches VS Parties\", y=1.02)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, it is pretty clear that both datasets are unbalanced. That means that the distribution of classes is skewed instead of uniform. The latter may cause us problem to the classification.\n",
    "\n",
    "From the plot above we can see that the number of speeches from 17-18 to 18-19  for all the parties is decreasing. For some of them, that decrease is really small(like C or SD) but for some other is significant high. The above can be an explanation, why the Social Democratic party and the moderate party had such a huge fall in the votes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train and evaluate a classifier. More specifically, we ask you to train a [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) classifier. You will have to\n",
    "\n",
    "1. vectorize the speeches in the training data\n",
    "2. instantiate and fit the Naive Bayes model\n",
    "3. evaluate the model on the test data\n",
    "\n",
    "The scikit-learn library provides a convenience class [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that allows you to solve the first two tasks with very compact code. For the evaluation you can use the function [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), which will report per-class precision, recall and F1, as well as overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impementation with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4280792420327304"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we have to vectorize the training and the test data\n",
    "#In that problem we will use count - Vectorization\n",
    "#It make more sense, because we will use a Bayes Classifier\n",
    "#Therefore if we have the tf-idf vec. the outputs will be close to zero and it will cause poor output\n",
    "#vectorizer = CountVectorizer()\n",
    "#X_train = vectorizer.fit_transform(training_data[\"words\"]) #first fit and transform\n",
    "#y_train = training_data[\"party\"]\n",
    "#X_test = vectorizer.transform(test_data[\"words\"]) #only transform\n",
    "#y_test = test_data[\"party\"]\n",
    "#Now we fit the multinomial bayessian model\n",
    "#clf = MultinomialNB()\n",
    "#clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "#make predictions\n",
    "#predictions = clf.predict(X_test)\n",
    "\n",
    "#print the classification report\n",
    "#print(classification_report(y_test, predictions))\n",
    "\n",
    "#Here we use pipeline for the preprocessing, fitting the classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "countVector = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "Pipl_classifier = Pipeline([('CountVec', countVector), ('Naive', clf)])\n",
    "\n",
    "Pipl_classifier.fit(training_data[\"words\"], training_data[\"party\"])\n",
    "\n",
    "\n",
    "#predictions with pipeline\n",
    "Pipl_classifier.score(test_data[\"words\"],test_data[\"party\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'S', 'S', ..., 'S', 'S', 'S'], dtype='<U2')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "In this task we implement a Multinomial Bayes classifier with default choice of parameters.\n",
    "\n",
    "We can now evaluate the classifier using the above results. The accuracy is approximately 43% which is quite low. One can say that we could expected that because of the unbalanced data. Accuracy is just a number though. In order to have a more clear picture, we should compare the accuracies of different models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics such as accuracy should not be understood as absolute measures of performance, but should be used only to compare different classifiers. When other classifiers are not available, a simple baseline for text classification is **Most Frequent Class (MFC)**. One way to think of this baseline is as a classifier that, for every document, predicts that class which appears most often in the training data.\n",
    "\n",
    "Determine the most frequent class in the 2017/2018 data. What is the accuracy of the MFC baseline on the test data? Given this baseline accuracy, how do you assess the results of the Naive Bayes classifier from Problem&nbsp;2? Answer with a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.298557278208441\n"
     ]
    }
   ],
   "source": [
    "#In this task we use a Baseline classifier\n",
    "#First we find the most frequent class of the training data\n",
    "training_data[\"party\"].value_counts().idxmax()\n",
    "\n",
    "#And we use that class in order to predict using MFC baseline\n",
    "MFC_accuracy = test_data[\"party\"].value_counts()[\"S\"]/test_data.shape[0]\n",
    "print(MFC_accuracy)\n",
    "\n",
    "#print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "It is obvious that a baseline classifier will give most of the times poor performance. In that case the accuracy of the MFC baseline is approximately 0.3 which is lower by 13% from the Bayes classifier. Therefore, it seems that the structure of the data causes problem in order to predict the classes correct. We can also comment that for the given problem, the accuracy of the Bayes classifier is quite good, even if the 43% seems low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Creating a balanced data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in Problem&nbsp;1, the distribution of the speeches over the eight different parties (classes) is imbalanced. One technique used to alleviate this is **undersampling**, in which one randomly removes samples from over-represented classes until all classes are represented with the same number of samples.\n",
    "\n",
    "Implement undersampling to create a balanced subset of the training data. Rerun the evaluation from Problem&nbsp;2 on the balanced data and compare the results. Discuss your findings in a short text. Would you argue that undersampling make sense for the task of predicting the party of a speaker?\n",
    "\n",
    "**Hint:** Your balanced subset should consist of 5,752 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H5-003-008</td>\n",
       "      <td>aktuell debatt om situationen för ensamkommand...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H5-003-016</td>\n",
       "      <td>herr talman jag lyssnar på migrationsministern...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H5-003-024</td>\n",
       "      <td>aktuell debatt om situationen för ensamkommand...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H5-004-031</td>\n",
       "      <td>herr talman jag har den senaste tiden haft anl...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H5-004-040</td>\n",
       "      <td>herr talman tack försvarsministern för det sva...</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>H5-126-017</td>\n",
       "      <td>fru talman penilla gunther lyfte ett antal vik...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5748</th>\n",
       "      <td>H5-117-015</td>\n",
       "      <td>fru talman lösningen på skolans utmaningar lig...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>H5-131-106</td>\n",
       "      <td>herr talman nu kommer jag att låta som en rikt...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>H5-023-044</td>\n",
       "      <td>fru talman om det är något som jag skulle vilj...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5751</th>\n",
       "      <td>H5-047-104</td>\n",
       "      <td>herr talman vi debatterar näringsutskottets be...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5752 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              words party\n",
       "0     H5-003-008  aktuell debatt om situationen för ensamkommand...     L\n",
       "1     H5-003-016  herr talman jag lyssnar på migrationsministern...     L\n",
       "2     H5-003-024  aktuell debatt om situationen för ensamkommand...     L\n",
       "3     H5-004-031  herr talman jag har den senaste tiden haft anl...     L\n",
       "4     H5-004-040  herr talman tack försvarsministern för det sva...     L\n",
       "...          ...                                                ...   ...\n",
       "5747  H5-126-017  fru talman penilla gunther lyfte ett antal vik...     V\n",
       "5748  H5-117-015  fru talman lösningen på skolans utmaningar lig...     V\n",
       "5749  H5-131-106  herr talman nu kommer jag att låta som en rikt...     V\n",
       "5750  H5-023-044  fru talman om det är något som jag skulle vilj...     V\n",
       "5751  H5-047-104  herr talman vi debatterar näringsutskottets be...     V\n",
       "\n",
       "[5752 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we implement undersampling\n",
    "#That means that we have to find the size of the less frequent class and resample the same size for all the other classes\n",
    "\n",
    "id_min = training_data.groupby(\"party\", as_index= False).count()[\"id\"].idxmin() #id of the less frequent class\n",
    "min_party = parties[id_min] #the less frequent class\n",
    "min_len = len(training_data[training_data[\"party\"] == min_party][\"party\"]) #the lenght of the less frequent class\n",
    "\n",
    "#We start with a df which contains only the less frequent party\n",
    "df = training_data[training_data[\"party\"] == min_party]\n",
    "pars = list(filter(lambda x: x!=\"L\",parties)) #go through the rest of the parties\n",
    "for par in pars:\n",
    "    \n",
    "    #From each class, we sample min_len samples and we add that in the data frame df\n",
    "    temp = training_data[training_data[\"party\"] == par].sample(min_len, random_state = 12345)\n",
    "    df = df.append(temp,ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.28      0.43      0.34       671\n",
      "          KD       0.32      0.39      0.35       821\n",
      "           L       0.28      0.43      0.34       560\n",
      "           M       0.40      0.51      0.45      1644\n",
      "          MP       0.35      0.39      0.37       809\n",
      "           S       0.80      0.28      0.42      2773\n",
      "          SD       0.43      0.42      0.43      1060\n",
      "           V       0.39      0.57      0.46       950\n",
      "\n",
      "    accuracy                           0.41      9288\n",
      "   macro avg       0.41      0.43      0.39      9288\n",
      "weighted avg       0.49      0.41      0.41      9288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Re run the classifier in the balanced data \n",
    "#First we have to vectorize the training and the test data\n",
    "X = df[\"words\"]\n",
    "y = df[\"party\"]\n",
    "X_test = test_data[\"words\"]\n",
    "y_test = test_data[\"party\"]\n",
    "\n",
    "countVector = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "Pipl_classifier = Pipeline([('CountVec', countVector), ('Naive', clf)])\n",
    "\n",
    "Pipl_classifier.fit(X, y)\n",
    "predictions_bal = Pipl_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#print the classification report\n",
    "print(classification_report(y_test, predictions_bal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we try to transform our inbalanced data into a balanced data using undersampling. As we can see from the classification report printed above, the accuracy of the classifier reduced from 43% to 41%. \n",
    "\n",
    "Undersampling is a really simple techinque but it can work efficient enough in many causes. In that case thought, it seems that that method can not help. A reason could be that we train our classifier in less data than before(5752 instead of 12343). Moreover, using the count Vectorization technique, the number of features we have is really high(75125). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is a specific table layout that is useful when analysing the performance of a classifier. In this matrix, both the rows and the columns correspond to classes, and each cell $(i, j)$ states how many times a sample with gold-standard class $i$ was predicted as belonging to class $j$.\n",
    "\n",
    "In scitkit-learn, the confusion matrix of a classifier is computed by the function [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "Your task is to use the confusion matrix to find, for each given party $p$ in the Riksdag, that other party $p'$ which the classifier that you trained in Problem&nbsp;4 most often confuses $p$ with when it predicts the party of a speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>KD</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>MP</th>\n",
       "      <th>S</th>\n",
       "      <th>SD</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>288</td>\n",
       "      <td>57</td>\n",
       "      <td>47</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KD</th>\n",
       "      <td>87</td>\n",
       "      <td>324</td>\n",
       "      <td>55</td>\n",
       "      <td>158</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>52</td>\n",
       "      <td>47</td>\n",
       "      <td>241</td>\n",
       "      <td>63</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>192</td>\n",
       "      <td>163</td>\n",
       "      <td>130</td>\n",
       "      <td>831</td>\n",
       "      <td>84</td>\n",
       "      <td>40</td>\n",
       "      <td>110</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MP</th>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>104</td>\n",
       "      <td>316</td>\n",
       "      <td>72</td>\n",
       "      <td>47</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>202</td>\n",
       "      <td>221</td>\n",
       "      <td>184</td>\n",
       "      <td>491</td>\n",
       "      <td>321</td>\n",
       "      <td>789</td>\n",
       "      <td>205</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>98</td>\n",
       "      <td>188</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>449</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>59</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>71</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C   KD    L    M   MP    S   SD    V\n",
       "C   288   57   47  105   47   18   55   54\n",
       "KD   87  324   55  158   33   24   66   74\n",
       "L    52   47  241   63   32   10   45   70\n",
       "M   192  163  130  831   84   40  110   94\n",
       "MP   60   56   57  104  316   72   47   97\n",
       "S   202  221  184  491  321  789  205  360\n",
       "SD   90   86   98  188   39   16  449   94\n",
       "V    53   65   59  113   35   16   71  538"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we will use the confusion matrix for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions_bal)\n",
    "\n",
    "conf_matrix = pd.DataFrame(conf_matrix,index=parties, columns=parties)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 'M'),\n",
       " ('KD', 'M'),\n",
       " ('L', 'V'),\n",
       " ('M', 'C'),\n",
       " ('MP', 'M'),\n",
       " ('S', 'M'),\n",
       " ('SD', 'M'),\n",
       " ('V', 'M')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comprehension\n",
    "res = [(k,v[v!=v[k]].idxmax()) for (k,v) in conf_matrix.iterrows()]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal of the confusion matrix gives us the proportion of each class which classified correctly. In that task, we went through each row in order to find the most common class for each missclassified class. \n",
    "\n",
    "In the list above, the first element in the tuple is the true label, and the second is the most common label when the class is misclassified. As we cna see in almost all the classes, the \"misleading\" class is \"M\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, you have been using the vectorizer and the Naive Bayes classifier with their default hyperparameters. When working with real-world applications, you would want to find settings for the hyperparameters that maximize the performance for the task at hand.\n",
    "\n",
    "Manually tweaking the hyperparameters of the various components of a vectorizer–classifier pipeline can be cumbersome. However, scikit-learn makes it possible to run an exhaustive search for the best hyperparameters over a grid of possible values. This method is known as **grid search**.\n",
    "\n",
    "The hyperparameters of a pipeline should never be tuned on the final test set. (Why would that be a bad idea?) Instead, one should either use a separate validation set, or run cross-validation over different folds. Here we will use cross-validation.\n",
    "\n",
    "Implement a grid search with 5-fold cross-validation to find the optimal parameters in a grid defined by the following choices for the hyperparameters:\n",
    "\n",
    "* In the vectorizer, try a set-of-words model instead of the default bag-of-words model (two possible parameter values).\n",
    "* Also in the vectorizer, try extracting $n$-grams up to $n = 2$ (two possible parameter values).\n",
    "* In the Naive Bayes classifier, try using additive smoothing with $\\alpha \\in \\{1, 0{.}1\\}$ (two possible parameter values).\n",
    "\n",
    "Use the class [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from the scikit-learn library. Print the results of your best model, along with the parameter values that yielded these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#We now use grid search for finding the optimal paramaters\n",
    "#We have to create the pipline in the same way as before\n",
    "countVector = CountVectorizer()\n",
    "clf = MultinomialNB()\n",
    "Pipl_classifier = Pipeline([('CountVec', countVector), \n",
    "                            ('Naive', clf)])\n",
    "\n",
    "#Now we will use the GridSearch \n",
    "#denote the parameters\n",
    "Cv = 5 #folds for cross validation\n",
    "\n",
    "#store the parameters we want to estimate\n",
    "parameters = {\n",
    "    \"CountVec__ngram_range\":[(1,1),(1,2),(2,2)], #nameOfStep__nameOfParameter\n",
    "    \"CountVec__binary\":[True,False],\n",
    "    \"Naive__alpha\":[0.1,1]\n",
    "}\n",
    "\n",
    "#store the gridsearch\n",
    "grid = GridSearchCV(Pipl_classifier, param_grid = parameters, cv = Cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('CountVec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('Naive',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'CountVec__binary': [True, False],\n",
       "                         'CountVec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "                         'Naive__alpha': [0.1, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the gridSearch\n",
    "grid.fit(training_data[\"words\"], training_data[\"party\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f511c189a32d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"words\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"party\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#grid.best_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Try to improve your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn makes it easy to test different vectorizer–classifier pipelines – among other things, it includes different types of logistic regression classifiers, support vector machines, and decision trees. Browse the library to see which methods are supported.\n",
    "\n",
    "Build a pipeline that you find interesting, and use grid search to find optimal settings for the hyperparameters. Print the results of your best model. Did you manage to get better results than the ones that you obtained in Problem&nbsp;5? Answer with a short text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp', 'delområd', 'maskininlärning']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "#def preprocess(text):\n",
    "    \n",
    "    #split the text into words\n",
    "    #tokens = word_tokenize(text)\n",
    "    \n",
    "    #convert all words in lower case\n",
    "    #tokens = [w.lower() for w in tokens]\n",
    "     \n",
    "    #After that we have to remove the punctuations like comma etc\n",
    "    #table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    #stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    #words = [word for word in stripped if word.isalpha()]\n",
    "     \n",
    "    #first we remove the most common words that they do not have any value such as eller or och\n",
    "    #words = [word for word in words if word not in stopwords.words(\"swedish\")]\n",
    "    \n",
    "    #Now it is time for steam the words\n",
    "    #words = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    #should return Array\n",
    "    #return words\n",
    "\n",
    "table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "stop_words = stopwords.words(\"swedish\")\n",
    "stemmer = SnowballStemmer(\"swedish\")\n",
    "def preprocess(text):\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    final_words = []\n",
    "    for word in tokens:\n",
    "        word = word.lower().translate(table)\n",
    "        \n",
    "        if word.isalpha() and word not in stop_words:\n",
    "            final_words.append(stemmer.stem(word))\n",
    "    return final_words\n",
    "\n",
    "input_text = \"NLP är ett delområde inom maskininlärning.\"\n",
    "preprocess(input_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-16990566f496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                                  \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"party\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                                  \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                  cv=CV)\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mentries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 232\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-4aef1cbb1880>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mfinal_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#First we have to do some model selection\n",
    "#We will try different models such as random forest, Linear SVC, Multinomail Bayes and Logistic Regression\n",
    "countVector = CountVectorizer(tokenizer=preprocess)\n",
    "pipelines = [\n",
    "    (\"Random_Forest\",\n",
    "     Pipeline([('CountVec', countVector), (\"Random_Forest\", RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0))])),\n",
    "    (\"LinearSVC\",\n",
    "     Pipeline([('CountVec', countVector), (\"LinearSVC\", LinearSVC())])),\n",
    "    (\"MultinomialNB\",\n",
    "     Pipeline([('CountVec', countVector), (\"MultinomialNB\", MultinomialNB())])),\n",
    "    (\"LogisticRegression\",\n",
    "     Pipeline([('CountVec', countVector), (\"LogisticRegression\", LogisticRegression(random_state=0))]))\n",
    "]\n",
    "\n",
    "\n",
    "CV = 5 #5-fold cross validation\n",
    "cv_df = pd.DataFrame(index=range(CV * len(pipelines))) #store the accyracies\n",
    "\n",
    "\n",
    "entries = []\n",
    "for (model_name,pip) in pipelines:\n",
    "    #model_name = model.__class__.__name__\n",
    "    \n",
    "    #in the vectorizer we exclude the most common words in swedish like och and eller.\n",
    "    accuracies = cross_val_score(pip, \n",
    "                                 training_data[\"words\"],\n",
    "                                 training_data[\"party\"],\n",
    "                                 scoring='accuracy',\n",
    "                                 cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization of the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineLogistic = pipelines[3][1]\n",
    "\n",
    "parameters = {\n",
    "    \"countVectorizer__ngram_range\": [(1,1), (1,2)],\n",
    "    \"countVectorizer__binary\": [True, False],\n",
    "    \"classifier__C\": [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridBest = GridSearchCV(pipelineLogistic, param_grid=parameters, cv=5)\n",
    "gridBest.fit(training_data[\"words\"], training_data[\"party\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to try other models, in order to improve our results. We will use 4 different models(Multinomial Bayes, Linear SVC, Random Forest and Logistic Regression).\n",
    "\n",
    "The first step is to do the model selection. For that, we use a 5-flold cross validation. Therefore, we calcualate the accuracies for all 4 models and we select the one with the highest accuracy. \n",
    "\n",
    "We now have the best model between those. It is time to see how good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Please read the section ‘General information’ on the ‘Labs’ page of the course website before submitting this notebook!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
